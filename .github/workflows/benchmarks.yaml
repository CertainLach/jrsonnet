name: Benchmarks

on:
  pull_request:
    branches: [main, master]
  workflow_dispatch:

permissions:
  contents: read

jobs:
  benchmark:
    name: ${{ matrix.benchmark.name }}
    runs-on: ubuntu-x64
    strategy:
      fail-fast: false
      matrix:
        benchmark:
          - name: "Env List"
            config: "rtk-benchmarks/env-list.yaml"
            id: "env-list"
          - name: "Eval"
            config: "rtk-benchmarks/eval.yaml"
            id: "eval"
          - name: "Export (Full)"
            config: "rtk-benchmarks/export-full.yaml"
            id: "export-full"
          - name: "Export (Replace)"
            config: "rtk-benchmarks/export-replace.yaml"
            id: "export-replace"
          - name: "Tool Importers"
            config: "rtk-benchmarks/tool-importers.yaml"
            id: "tool-importers"
          - name: "Tool Imports"
            config: "rtk-benchmarks/tool-imports.yaml"
            id: "tool-imports"
          - name: "Tool Importers Count"
            config: "rtk-benchmarks/tool-importers-count.yaml"
            id: "tool-importers-count"
    steps:
      - name: Checkout
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.4
        with:
          # Fetch full history to allow building rtk from base branch
          fetch-depth: 0

      - name: Set up Rust toolchain
        uses: actions-rust-lang/setup-rust-toolchain@1780873c7b576612439a134613cc4cc74ce5538c # v1.15.2

      - name: Install Tanka
        run: |
          curl -fSL -o "/usr/local/bin/tk" "https://github.com/grafana/tanka/releases/latest/download/tk-linux-amd64"
          chmod +x /usr/local/bin/tk
          tk --version

      - name: Install dependencies
        run: |
          # Install hyperfine
          wget https://github.com/sharkdp/hyperfine/releases/download/v1.18.0/hyperfine_1.18.0_amd64.deb
          sudo dpkg -i hyperfine_1.18.0_amd64.deb
          # Install uv for Python script
          curl -LsSf https://astral.sh/uv/install.sh | sh
          # jq is pre-installed on ubuntu-latest

      - name: Run benchmark
        env:
          BENCHMARK_MARKDOWN_OUTPUT: benchmark-results.md
          BENCHMARK_SUMMARY_OUTPUT: benchmark-summary.json
          # On PRs, also build and benchmark rtk from the base branch (master)
          BENCHMARK_BASE_REF: ${{ github.event_name == 'pull_request' && github.event.pull_request.base.ref || '' }}
        run: |
          # Use pipefail to ensure the exit code from the benchmark script is preserved
          set -o pipefail
          ./rtk-benchmarks/run-benchmark.py ${{ matrix.benchmark.config }} | tee benchmark-results.md

      - name: Upload benchmark output
        if: always()
        uses: actions/upload-artifact@26f96dfa697d77e81fd5907df203aa23a56210a8 # v4
        with:
          name: benchmark-${{ matrix.benchmark.id }}
          path: |
            benchmark-results.md
            benchmark-summary.json
          if-no-files-found: warn

  summary:
    name: Post Summary Comment
    runs-on: ubuntu-latest
    needs: benchmark
    if: always() && github.event_name == 'pull_request'
    permissions:
      pull-requests: write
      issues: write
    steps:
      - name: Download all benchmark artifacts
        uses: actions/download-artifact@fa0a91b85d4f404e444e00e005971372dc801d16 # v4
        with:
          pattern: benchmark-*
          path: artifacts

      - name: Generate summary comment
        id: summary
        run: |
          # Build the comment body
          {
            echo "## Benchmark Results"
            echo ""
            echo "| Benchmark | Test | vs tk | vs base |"
            echo "|:----------|:-----|------:|--------:|"
            
            # Process each benchmark summary and generate table rows
            for dir in artifacts/benchmark-*/; do
              if [ -f "$dir/benchmark-summary.json" ]; then
                jq -r '
                  .benchmark_name as $bench |
                  .tests[] |
                  "| \($bench) | \(.name) | \(.vs_tk)x faster | \(.vs_base // "-") |"
                ' "$dir/benchmark-summary.json"
              fi
            done
            
            echo ""
            echo "_Full results available in [workflow artifacts](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})._"
            echo ""
            echo "_Benchmark run on commit \`${{ github.event.pull_request.head.sha }}\`_"
            echo "<!-- BENCHMARK_SUMMARY -->"
          } > comment-body.txt
          
          # Output for next step
          {
            echo "body<<GITHUB_OUTPUT_EOF"
            cat comment-body.txt
            echo "GITHUB_OUTPUT_EOF"
          } >> $GITHUB_OUTPUT

      - name: Post PR comment
        uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea # v7
        env:
          COMMENT_BODY: ${{ steps.summary.outputs.body }}
        with:
          script: |
            const commentMarker = '<!-- BENCHMARK_SUMMARY -->';
            const comment = process.env.COMMENT_BODY;
            
            // Find existing benchmark summary comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const botComment = comments.find(c => 
              c.user.type === 'Bot' && 
              c.body.includes(commentMarker)
            );
            
            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
              console.log('Updated existing benchmark summary comment');
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
              console.log('Created new benchmark summary comment');
            }
